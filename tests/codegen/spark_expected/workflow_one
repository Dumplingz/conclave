import pyspark.sql as psql
from pyspark.conf import SparkConf
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from pyspark.sql import functions as F
from pyspark.sql.window import Window
import numpy, functools, operator, sys

conf = SparkConf()
sp = psql \
    .SparkSession \
    .builder.config(conf=conf) \
    .appName("code") \
    .getOrCreate()

def union_all(dfs):
    return functools.reduce(psql.DataFrame.unionAll, dfs)

inpt_idx = 1


schema = StructType([StructField('a', IntegerType(), True),StructField('b', IntegerType(), True),StructField('c', IntegerType(), True),StructField('d', IntegerType(), True)])

in_1 = sp.read.csv(
    '/tmp/in_1.csv',
    schema=schema,
    header=True) \
    


div = in_1 \
    .withColumn('a', in_1.a/in_1.b/1) \
    


schema = StructType([StructField('a', IntegerType(), True),StructField('b', IntegerType(), True),StructField('c', IntegerType(), True),StructField('d', IntegerType(), True)])

in_2 = sp.read.csv(
    '/tmp/in_2.csv',
    schema=schema,
    header=True) \
    


mult = in_2 \
    .withColumn('a', in_2.a*in_2.b*1) \
    


proj = div \
    .select(['a', 'b']) \
    

join = proj \
    .join(mult, ['a', 'b']) \
    


agg = join \
    .groupBy('a','b') \
    .agg({'c':'sum'}) \
    .withColumnRenamed('sum(c)', 'agg_1') \
    


agg \
    .write \
    .csv("/tmp/agg.csv")



sp.stop()

